{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian regression\n",
    "\n",
    "## Kernel\n",
    "\n",
    "Kernel functions specify the **similarity** between any two data points.\n",
    "The kernel functions encode **assumptions** about the function from which we learn : for example it's smoothness.\n",
    "\n",
    "**Different kernels** -> VERY DIFFERENT MODELS\n",
    "\n",
    "**Large freedom** in selecting the optimal kernel function for some specific application.\n",
    "\n",
    "$ \\text{Kernel functions } k(x, x') \\text{ must satisfy (cf. properties of covariance matrices):} $\n",
    "\n",
    "1. $ \\textbf{Symmetry:} \\; k(x, x') = k(x', x) $\n",
    "\n",
    "2. $ \\textbf{Positive semi-definiteness (continuous case):} $\n",
    "\n",
    "$$\n",
    "\\int_{\\Omega} k(x, x') f(x) f(x') \\, dx \\, dx' \\geq 0 \\quad \\forall f \\in L_2, \\, \\Omega \\subset \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "Kernels represent scalar products in corresponding Euclidean spaces. Such spaces could be very high-dimensional (even infinite dimensional) but the evaluation of the kernel yields the correct result for the scalar product without constructing explicit vector representations.\n",
    "\n",
    "### Gram matrix (Discrete case)\n",
    "\n",
    "\n",
    "Corresponding condition for a kernel function to be **positive semi-definite** (psd):\n",
    "\n",
    "\n",
    "For any $n \\in \\mathbb{N}$, any set $S = \\{x_1, \\dots, x_n\\}$, the kernel (Gram) matrix\n",
    "\n",
    "$$\n",
    "\\mathbf{K} = \\begin{pmatrix}\n",
    "k(x_1, x_1) & \\dots & k(x_1, x_n) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "k(x_n, x_1) & \\dots & k(x_n, x_n)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "must be positive semi-definite.\n",
    "\n",
    "$$\n",
    "\\text{Remember: } \\mathbf{K} \\text{ is positive semi-definite} \\iff \\mathbf{x}^\\top \\mathbf{K} \\mathbf{x} \\geq 0 \\text{ for all } x_i\n",
    "$$\n",
    "\n",
    "# Kernel function\n",
    "\n",
    "A function $ k : \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R} $ is said to be a kernel function \n",
    "if and only if it is an inner product $\\langle \\phi(x), \\phi(x') \\rangle$ for some (possibly infinite \n",
    "dimensional) mapping $\\phi(x)$.\n",
    "\n",
    "We rewrite the joint distribution over $\\mathbf{y}$ as\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    "\\sim \\mathcal{N} \\left( \\mathbf{y} \\, 0, \\,\n",
    "\\begin{bmatrix}\n",
    "k_{1,1} + \\sigma^2 & k_{1,2} & \\cdots & k_{1,n} \\\\\n",
    "k_{2,1} & k_{2,2} + \\sigma^2 & \\cdots & k_{2,n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "k_{n,1} & k_{n,2} & \\cdots & k_{n,n} + \\sigma^2\n",
    "\\end{bmatrix}\n",
    "\\right),\n",
    "$$\n",
    "with $k_{i,j} = k(x_i, x_j) := x_i^\\top \\Lambda^{-1} x_j$ being a $\\textit{kernel function}$.\n",
    "\n",
    "# Make a prediction\n",
    "\n",
    "The key assumption in Gaussian Processes is that f(x) and f(x^âˆ—) are jointly\n",
    "distributed as a multivariate normal\n",
    "\n",
    "\n",
    "Once we have the covariance matrix\n",
    "\n",
    "To make predictions for the points $x^* = \\{40, 60\\}$, you need to use the joint Gaussian distribution of $f(x)$ and $f(x^*)$ as given in the exercise:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "f(x) \\\\\n",
    "f(x^*)\n",
    "\\end{pmatrix} \\sim \\mathcal{N}(0, \\Sigma)\n",
    "$$\n",
    "where $\\Sigma$ is the covariance matrix you filled out.\n",
    "\n",
    "### General Steps:\n",
    "\n",
    "1. **Partition the covariance matrix**: Separate the covariance matrix $\\Sigma$ into blocks based on the observed points $x = \\{20, 100\\}$ and the test points $x^* = \\{40, 60\\}$:\n",
    "\n",
    "   $$\n",
    "   \\Sigma =\n",
    "   \\begin{pmatrix}\n",
    "   \\Sigma_{xx} & \\Sigma_{xx^*} \\\\\n",
    "   \\Sigma_{x^*x} & \\Sigma_{x^*x^*}\n",
    "   \\end{pmatrix}\n",
    "   $$\n",
    "\n",
    "   Where:\n",
    "   - $\\Sigma_{xx}$ is the covariance between the observed points.\n",
    "   - $\\Sigma_{xx^*}$ is the covariance between the observed points and the test points.\n",
    "   - $\\Sigma_{x^*x}$ is the transpose of $\\Sigma_{xx^*}$.\n",
    "   - $\\Sigma_{x^*x^*}$ is the covariance between the test points.\n",
    "\n",
    "2. **Mean and covariance of the posterior distribution**: The predictive distribution for $f(x^*)$, given the observed values $f(x)$, is also a Gaussian distribution with mean and covariance given by:\n",
    "\n",
    "   $$\n",
    "   \\mathbb{E}[f(x^*)] = \\Sigma_{x^*x} \\Sigma_{xx}^{-1} f(x)\n",
    "   $$\n",
    "   $$\n",
    "   \\text{Cov}(f(x^*)) = \\Sigma_{x^*x^*} - \\Sigma_{x^*x} \\Sigma_{xx}^{-1} \\Sigma_{xx^*}\n",
    "   $$\n",
    "\n",
    "3. **Plug in the values**: \n",
    "   - Use the observed values $f(x) = \\{10, 40\\}$ for the observed points $x = \\{20, 100\\}$.\n",
    "   - $\\Sigma_{xx}$, $\\Sigma_{xx^*}$, and $\\Sigma_{x^*x^*}$ can be extracted from the full covariance matrix you already computed.\n",
    "\n",
    "4. **Compute the mean**: Multiply $\\Sigma_{x^*x}$ by $\\Sigma_{xx}^{-1}$ and then multiply by $f(x)$ to get the predictive mean for $f(x^*)$.\n",
    "\n",
    "5. **Compute the covariance**: Calculate the predictive covariance matrix using the formula above.\n",
    "\n",
    "This will give you the predicted distribution for the test points $x^* = \\{40, 60\\}$, including the mean and uncertainty (covariance).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "The SVD of a matrix $X$ is given by:\n",
    "\n",
    "$$ X = U \\Sigma V^\\top $$\n",
    "\n",
    "where:\n",
    "- $U$ is a matrix of the left singular vectors,\n",
    "- $\\Sigma$ is a diagonal matrix of singular values,\n",
    "- $V^\\top$ is the transpose of the matrix of right singular vectors.\n",
    "\n",
    "#### Transpose of $X$\n",
    "\n",
    "Taking the transpose of $X$ gives:\n",
    "\n",
    "$$ X^\\top = V \\Sigma^\\top U^\\top $$\n",
    "\n",
    "#### Correlation Matrix\n",
    "\n",
    "The matrix $X^\\top X$ represents the correlation matrix. This matrix is often used in principal component analysis (PCA) and related methods. We have:\n",
    "\n",
    "$$ X^\\top X = V \\Sigma^\\top U^\\top U \\Sigma V^\\top $$\n",
    "\n",
    "Since $U^\\top U = I$ (the identity matrix, because $U$ is orthogonal), this simplifies to:\n",
    "\n",
    "$$ X^\\top X = V \\Sigma^\\top \\Sigma V^\\top = V \\Sigma^2 V^\\top $$\n",
    "\n",
    "This is an important result because it shows that the matrix $X^\\top X$ can be diagonalized using the matrix $V$. The diagonal elements of $\\Sigma^2$ represent the squared singular values, which are also the eigenvalues of $X^\\top X$.\n",
    "\n",
    "#### Eigenvalue Decomposition of $X^\\top X$\n",
    "\n",
    "From the previous equation, we see that $X^\\top X$ can be written as:\n",
    "\n",
    "$$ X^\\top X V = V \\Sigma^2 $$\n",
    "\n",
    "This shows that the columns of $V$ are the eigenvectors of $X^\\top X$, and the diagonal elements of $\\Sigma^2$ are the eigenvalues.\n",
    "\n",
    "#### $XX^\\top$ Matrix\n",
    "\n",
    "Similarly, for the matrix $XX^\\top$, we have:\n",
    "\n",
    "$$ XX^\\top = U \\Sigma V^\\top V \\Sigma^\\top U^\\top $$\n",
    "\n",
    "Again, since $V^\\top V = I$, this simplifies to:\n",
    "\n",
    "$$ XX^\\top = U \\Sigma^2 U^\\top $$\n",
    "\n",
    "This shows that the matrix $XX^\\top$ is diagonalized by $U$, with eigenvalues given by the diagonal elements of $\\Sigma^2$.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- $X^\\top X = V \\Sigma^2 V^\\top$ shows that $V$ contains the eigenvectors of $X^\\top X$, and $\\Sigma^2$ contains the eigenvalues.\n",
    "- $XX^\\top = U \\Sigma^2 U^\\top$ shows that $U$ contains the eigenvectors of $XX^\\top$, and $\\Sigma^2$ contains the eigenvalues.\n",
    "\n",
    "In both cases, the matrix $\\Sigma^2$ represents the squared singular values, which correspond to the eigenvalues of both $X^\\top X$ and $XX^\\top$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge & Lasso regression\n",
    "\n",
    "### Ridge Regression\n",
    "Ridge regression adds a regularization term to the ordinary least squares (OLS) loss function. The objective function is:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\left\\{ \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\beta)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\right\\}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $y_i$ are the true values,\n",
    "- $\\mathbf{x}_i$ are the feature vectors,\n",
    "- $\\beta$ are the coefficients,\n",
    "- $\\lambda$ is the regularization parameter.\n",
    "\n",
    "### Lasso Regression\n",
    "Lasso regression uses an L1 regularization term, which encourages sparsity in the coefficients. The objective function is:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\left\\{ \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\beta)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\right\\}\n",
    "$$\n",
    "\n",
    "where **the terms are the same as those in the Ridge Regression equation, but with an L1 norm** ($|\\beta_j|$) for the regularization term.\n",
    "\n",
    "\n",
    "## Remarks\n",
    "\n",
    "**Lasso Regression** can exclude useless variation from equations, by tuning the weights **ALL THE WAY TO ZERO**\n",
    "\n",
    "**Ridge regression** can only asymptotically tune the weights to zero\n",
    "\n",
    "So Lasso is better than Ridge at **reducing variance** in models with a **lot of useless variables**\n",
    "\n",
    "Ridge regression tends to better when all variable are useful\n",
    "\n",
    "# Gauss-Markov Theorem\n",
    "\n",
    "For any linear estimator $\\hat{\\theta} = c^\\top \\mathbf{y}$ that is unbiased for $a^\\top \\beta$, we have\n",
    "\n",
    "$$\n",
    "\\mathbb{V}(a^\\top \\hat{\\beta}) \\leq \\mathbb{V}(c^\\top \\mathbf{y}).\n",
    "$$\n",
    "C.f. exercise week 4-5 for proof\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum A Posteriori (MAP) Estimator\n",
    "\n",
    "The Maximum A Posteriori (MAP) estimator is given by:\n",
    "\n",
    "$$ \\hat{\\theta}_{MAP} = \\underset{\\theta}{\\mathrm{arg\\,max}} \\, P(\\theta | \\mathbf{x}) $$\n",
    "\n",
    "Using Bayes' theorem, this can be rewritten as:\n",
    "\n",
    "$$ \\hat{\\theta}_{MAP} = \\underset{\\theta}{\\mathrm{arg\\,max}} \\, \\left[ P(\\mathbf{x} | \\theta) P(\\theta) \\right] $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ P(\\theta | \\mathbf{x}) $ is the posterior probability of the parameter $ \\theta $ given the data $ \\mathbf{x} $,\n",
    "- $ P(\\mathbf{x} | \\theta) $ is the likelihood of the data given $ \\theta $,\n",
    "- $ P(\\theta) $ is the prior distribution of $ \\theta $.\n",
    "\n",
    "### Maximum Likelihood Estimator (MLE)\n",
    "\n",
    "The Maximum Likelihood Estimator (MLE) is given by:\n",
    "\n",
    "$$ \\hat{\\theta}_{MLE} = \\underset{\\theta}{\\mathrm{arg\\,max}} \\, P(\\mathbf{x} | \\theta) $$\n",
    "\n",
    "In other words, MLE estimates the parameter $ \\theta $ that maximizes the likelihood of the observed data $ \\mathbf{x} $.\n",
    "\n",
    "### Relationship between MAP and MLE\n",
    "\n",
    "If the prior $ P(\\theta) $ is uniform (i.e., it does not depend on $ \\theta $), then the MAP estimator simplifies to the MLE:\n",
    "\n",
    "$$ \\hat{\\theta}_{MAP} = \\hat{\\theta}_{MLE} $$\n",
    "\n",
    "#### Remarks\n",
    "\n",
    "\n",
    "- MLE **often overfits**\n",
    "- MAP **avoids overfitting** -> Regularization/ Shrinkage\n",
    "- As n $ \\rightarrow \\infty $, MAP tends to look like MLE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an inline equation: $y = mx + b$.\n",
    "\n",
    "This is a block (display) equation:\n",
    "$$ y = mx + b $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance of a vector $ \\mathbf{x} $ multiplied by a matrix $ \\mathbf{M} $ is given by:\n",
    "\n",
    "$$ \\text{Var}(\\mathbf{M} \\mathbf{x}) $$\n",
    "\n",
    "Since variance is a quadratic form, we can factor the matrix $ \\mathbf{M} $ out of the variance as follows:\n",
    "\n",
    "$$ \\text{Var}(\\mathbf{M} \\mathbf{x}) = \\mathbf{M} \\, \\text{Var}(\\mathbf{x}) \\, \\mathbf{M}^\\top $$\n",
    "\n",
    "Here, $ \\text{Var}(\\mathbf{x}) $ is the covariance matrix of the vector $ \\mathbf{x} $, and $ \\mathbf{M}^\\top $ is the transpose of the matrix $ \\mathbf{M} $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jensen's Inequality\n",
    "\n",
    "Jensen's inequality applies to a convex function and is given by the following statement:\n",
    "\n",
    "For a convex function $f$, and for any random variable $X$:\n",
    "\n",
    "$$ f\\left( \\mathbb{E}[X] \\right) \\leq \\mathbb{E}[ f(X) ] $$\n",
    "\n",
    "where:\n",
    "- $f$ is a convex function,\n",
    "- $\\mathbb{E}[X]$ is the expected value of the random variable $X$,\n",
    "- $\\mathbb{E}[ f(X) ]$ is the expected value of the function $f$ applied to $X$.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- If $f$ is **concave**, then Jensen's inequality is reversed:\n",
    "\n",
    "$$ f\\left( \\mathbb{E}[X] \\right) \\geq \\mathbb{E}[ f(X) ] $$\n",
    "\n",
    "Jensen's inequality expresses that the value of a convex function at the expected value of a random variable is less than or equal to the expected value of the function applied to the random variable.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
