{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "The SVD of a matrix $X$ is given by:\n",
    "\n",
    "$$ X = U \\Sigma V^\\top $$\n",
    "\n",
    "where:\n",
    "- $U$ is a matrix of the left singular vectors,\n",
    "- $\\Sigma$ is a diagonal matrix of singular values,\n",
    "- $V^\\top$ is the transpose of the matrix of right singular vectors.\n",
    "\n",
    "#### Transpose of $X$\n",
    "\n",
    "Taking the transpose of $X$ gives:\n",
    "\n",
    "$$ X^\\top = V \\Sigma^\\top U^\\top $$\n",
    "\n",
    "#### Correlation Matrix\n",
    "\n",
    "The matrix $X^\\top X$ represents the correlation matrix. This matrix is often used in principal component analysis (PCA) and related methods. We have:\n",
    "\n",
    "$$ X^\\top X = V \\Sigma^\\top U^\\top U \\Sigma V^\\top $$\n",
    "\n",
    "Since $U^\\top U = I$ (the identity matrix, because $U$ is orthogonal), this simplifies to:\n",
    "\n",
    "$$ X^\\top X = V \\Sigma^\\top \\Sigma V^\\top = V \\Sigma^2 V^\\top $$\n",
    "\n",
    "This is an important result because it shows that the matrix $X^\\top X$ can be diagonalized using the matrix $V$. The diagonal elements of $\\Sigma^2$ represent the squared singular values, which are also the eigenvalues of $X^\\top X$.\n",
    "\n",
    "#### Eigenvalue Decomposition of $X^\\top X$\n",
    "\n",
    "From the previous equation, we see that $X^\\top X$ can be written as:\n",
    "\n",
    "$$ X^\\top X V = V \\Sigma^2 $$\n",
    "\n",
    "This shows that the columns of $V$ are the eigenvectors of $X^\\top X$, and the diagonal elements of $\\Sigma^2$ are the eigenvalues.\n",
    "\n",
    "#### $XX^\\top$ Matrix\n",
    "\n",
    "Similarly, for the matrix $XX^\\top$, we have:\n",
    "\n",
    "$$ XX^\\top = U \\Sigma V^\\top V \\Sigma^\\top U^\\top $$\n",
    "\n",
    "Again, since $V^\\top V = I$, this simplifies to:\n",
    "\n",
    "$$ XX^\\top = U \\Sigma^2 U^\\top $$\n",
    "\n",
    "This shows that the matrix $XX^\\top$ is diagonalized by $U$, with eigenvalues given by the diagonal elements of $\\Sigma^2$.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- $X^\\top X = V \\Sigma^2 V^\\top$ shows that $V$ contains the eigenvectors of $X^\\top X$, and $\\Sigma^2$ contains the eigenvalues.\n",
    "- $XX^\\top = U \\Sigma^2 U^\\top$ shows that $U$ contains the eigenvectors of $XX^\\top$, and $\\Sigma^2$ contains the eigenvalues.\n",
    "\n",
    "In both cases, the matrix $\\Sigma^2$ represents the squared singular values, which correspond to the eigenvalues of both $X^\\top X$ and $XX^\\top$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge & Lasso regression\n",
    "\n",
    "### Ridge Regression\n",
    "Ridge regression adds a regularization term to the ordinary least squares (OLS) loss function. The objective function is:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\left\\{ \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\beta)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\right\\}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $y_i$ are the true values,\n",
    "- $\\mathbf{x}_i$ are the feature vectors,\n",
    "- $\\beta$ are the coefficients,\n",
    "- $\\lambda$ is the regularization parameter.\n",
    "\n",
    "### Lasso Regression\n",
    "Lasso regression uses an L1 regularization term, which encourages sparsity in the coefficients. The objective function is:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\left\\{ \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\beta)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\right\\}\n",
    "$$\n",
    "\n",
    "where **the terms are the same as those in the Ridge Regression equation, but with an L1 norm** ($|\\beta_j|$) for the regularization term.\n",
    "\n",
    "\n",
    "## Remarks\n",
    "\n",
    "**Lasso Regression** can exclude useless variation from equations, by tuning the weights **ALL THE WAY TO ZERO**\n",
    "\n",
    "**Ridge regression** can only asymptotically tune the weights to zero\n",
    "\n",
    "So Lasso is better than Ridge at **reducing variance** in models with a **lot of useless variables**\n",
    "\n",
    "Ridge regression tends to better when all variable are useful\n",
    "\n",
    "# Gauss-Markov Theorem\n",
    "\n",
    "For any linear estimator $\\hat{\\theta} = c^\\top \\mathbf{y}$ that is unbiased for $a^\\top \\beta$, we have\n",
    "\n",
    "$$\n",
    "\\mathbb{V}(a^\\top \\hat{\\beta}) \\leq \\mathbb{V}(c^\\top \\mathbf{y}).\n",
    "$$\n",
    "C.f. exercise week 4-5 for proof\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum A Posteriori (MAP) Estimator\n",
    "\n",
    "The Maximum A Posteriori (MAP) estimator is given by:\n",
    "\n",
    "$$ \\hat{\\theta}_{MAP} = \\underset{\\theta}{\\mathrm{arg\\,max}} \\, P(\\theta | \\mathbf{x}) $$\n",
    "\n",
    "Using Bayes' theorem, this can be rewritten as:\n",
    "\n",
    "$$ \\hat{\\theta}_{MAP} = \\underset{\\theta}{\\mathrm{arg\\,max}} \\, \\left[ P(\\mathbf{x} | \\theta) P(\\theta) \\right] $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ P(\\theta | \\mathbf{x}) $ is the posterior probability of the parameter $ \\theta $ given the data $ \\mathbf{x} $,\n",
    "- $ P(\\mathbf{x} | \\theta) $ is the likelihood of the data given $ \\theta $,\n",
    "- $ P(\\theta) $ is the prior distribution of $ \\theta $.\n",
    "\n",
    "### Maximum Likelihood Estimator (MLE)\n",
    "\n",
    "The Maximum Likelihood Estimator (MLE) is given by:\n",
    "\n",
    "$$ \\hat{\\theta}_{MLE} = \\underset{\\theta}{\\mathrm{arg\\,max}} \\, P(\\mathbf{x} | \\theta) $$\n",
    "\n",
    "In other words, MLE estimates the parameter $ \\theta $ that maximizes the likelihood of the observed data $ \\mathbf{x} $.\n",
    "\n",
    "### Relationship between MAP and MLE\n",
    "\n",
    "If the prior $ P(\\theta) $ is uniform (i.e., it does not depend on $ \\theta $), then the MAP estimator simplifies to the MLE:\n",
    "\n",
    "$$ \\hat{\\theta}_{MAP} = \\hat{\\theta}_{MLE} $$\n",
    "\n",
    "#### Remarks\n",
    "\n",
    "\n",
    "- MLE **often overfits**\n",
    "- MAP **avoids overfitting** -> Regularization/ Shrinkage\n",
    "- As n $ \\rightarrow \\infty $, MAP tends to look like MLE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an inline equation: $y = mx + b$.\n",
    "\n",
    "This is a block (display) equation:\n",
    "$$ y = mx + b $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance of a vector $ \\mathbf{x} $ multiplied by a matrix $ \\mathbf{M} $ is given by:\n",
    "\n",
    "$$ \\text{Var}(\\mathbf{M} \\mathbf{x}) $$\n",
    "\n",
    "Since variance is a quadratic form, we can factor the matrix $ \\mathbf{M} $ out of the variance as follows:\n",
    "\n",
    "$$ \\text{Var}(\\mathbf{M} \\mathbf{x}) = \\mathbf{M} \\, \\text{Var}(\\mathbf{x}) \\, \\mathbf{M}^\\top $$\n",
    "\n",
    "Here, $ \\text{Var}(\\mathbf{x}) $ is the covariance matrix of the vector $ \\mathbf{x} $, and $ \\mathbf{M}^\\top $ is the transpose of the matrix $ \\mathbf{M} $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jensen's Inequality\n",
    "\n",
    "Jensen's inequality applies to a convex function and is given by the following statement:\n",
    "\n",
    "For a convex function $f$, and for any random variable $X$:\n",
    "\n",
    "$$ f\\left( \\mathbb{E}[X] \\right) \\leq \\mathbb{E}[ f(X) ] $$\n",
    "\n",
    "where:\n",
    "- $f$ is a convex function,\n",
    "- $\\mathbb{E}[X]$ is the expected value of the random variable $X$,\n",
    "- $\\mathbb{E}[ f(X) ]$ is the expected value of the function $f$ applied to $X$.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- If $f$ is **concave**, then Jensen's inequality is reversed:\n",
    "\n",
    "$$ f\\left( \\mathbb{E}[X] \\right) \\geq \\mathbb{E}[ f(X) ] $$\n",
    "\n",
    "Jensen's inequality expresses that the value of a convex function at the expected value of a random variable is less than or equal to the expected value of the function applied to the random variable.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
